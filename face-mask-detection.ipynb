{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Mask Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Packages\n",
    "\n",
    "1. Install PyTorch. (If you want to use CUDA, you need to install it manully first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyTorch (CUDA): https://pytorch.org/get-started/locally/\n",
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Install required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install numpy opencv-python ultralytics matplotlib pyyaml scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and Setup Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup data and folder structure for YOLO to train with:\n",
    "\n",
    "```\n",
    "|-- dataset\n",
    "`-- original\n",
    "    |-- original\n",
    "    |   |-- annotations\n",
    "    |   `-- images\n",
    "    `-- yolo\n",
    "        |-- images\n",
    "        |   |-- test\n",
    "        |   |-- train\n",
    "        |   `-- val\n",
    "        `-- labels\n",
    "            |-- test\n",
    "            |-- train\n",
    "            `-- val\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "cwd = Path.cwd()\n",
    "\n",
    "DATASET_PATH = cwd / 'dataset'\n",
    "\n",
    "ORIGINAL_DATASET_PATH = DATASET_PATH / 'original'\n",
    "\n",
    "ORIGINAL_IMAGES_PATH = ORIGINAL_DATASET_PATH / 'images'\n",
    "ORIGINAL_ANNOTATIONS_PATH = ORIGINAL_DATASET_PATH / 'annotations'\n",
    "\n",
    "YOLO_DATASET_PATH = DATASET_PATH / 'yolo'\n",
    "\n",
    "TRAIN_DIR = 'train'\n",
    "TRAIN_IMAGES_PATH = YOLO_DATASET_PATH / 'images' / TRAIN_DIR\n",
    "TRAIN_LABELS_PATH = YOLO_DATASET_PATH / 'labels' / TRAIN_DIR\n",
    "\n",
    "TEST_DIR = 'test'\n",
    "TEST_IMAGES_PATH = YOLO_DATASET_PATH / 'images' / TEST_DIR\n",
    "TEST_LABELS_PATH = YOLO_DATASET_PATH / 'labels' / TEST_DIR\n",
    "\n",
    "VALIDATION_DIR = 'val'\n",
    "VALIDATION_IMAGES_PATH = YOLO_DATASET_PATH / 'images' / VALIDATION_DIR\n",
    "VALIDATION_LABELS_PATH = YOLO_DATASET_PATH / 'labels' / VALIDATION_DIR\n",
    "\n",
    "for dir in [ORIGINAL_DATASET_PATH,\n",
    "            TRAIN_IMAGES_PATH, TRAIN_LABELS_PATH,\n",
    "            TEST_IMAGES_PATH, TEST_LABELS_PATH,\n",
    "            VALIDATION_IMAGES_PATH, VALIDATION_LABELS_PATH]:\n",
    "    # create directories if not exist\n",
    "    os.makedirs(dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download dataset from [Kaggle's Face Mask Detection Dataset](https://www.kaggle.com/datasets/omkargurav/face-mask-dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "\n",
    "DATASET_FILE_PATH = DATASET_PATH / 'archive.zip'\n",
    "\n",
    "# https://www.kaggle.com/datasets/omkargurav/face-mask-dataset\n",
    "DATASET_URL = 'https://www.kaggle.com/api/v1/datasets/download/andrewmvd/face-mask-detection'\n",
    "\n",
    "urlretrieve(DATASET_URL, DATASET_FILE_PATH)\n",
    "\n",
    "print('dataset downloaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracts it to `dataset/original/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "# extracts all files to cwd. which should only create folder `annotations` and `images`.\n",
    "with zipfile.ZipFile(DATASET_FILE_PATH, 'r') as f:\n",
    "    f.extractall(ORIGINAL_DATASET_PATH)\n",
    "\n",
    "print('dataset extracted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepares data for YOLO to convert from Pascal VOC to YOLO format.\n",
    "\n",
    "Get a list of all annotation files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all files which are XMLs in the annotations folder.\n",
    "label_files = [ORIGINAL_ANNOTATIONS_PATH / f\n",
    "               for f in os.listdir(ORIGINAL_ANNOTATIONS_PATH)\n",
    "               if Path(f).suffix.lower() == '.xml']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to convert Pascal VOC's annotation format to a YOLO one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xml_to_yolo_bbox(bbox, w, h):\n",
    "    \"\"\"Convert Pascal VOC's annotation to YOLO's format\"\"\"\n",
    "    xmin, ymin, xmax, ymax = bbox\n",
    "\n",
    "    x_center = (xmax + xmin) / 2 / w\n",
    "    y_center = (ymax + ymin) / 2 / h\n",
    "\n",
    "    width = (xmax - xmin) / w\n",
    "    height = (xmax - xmin) / h\n",
    "\n",
    "    return [x_center, y_center, width, height]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Parse all Pascal VOC annotations. (which are XMLs)\n",
    "\n",
    "2. Get all essential values. (width, height, classes, bounding boxes)\n",
    "\n",
    "3. Convert to YOLO format with function `xml_to_yolo_bbox`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "classes = []\n",
    "\n",
    "images = dict()\n",
    "labels = dict()\n",
    "\n",
    "for label_file in label_files:\n",
    "    # parse the xml file\n",
    "    tree = ET.parse(label_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # get `filename` and checks the image is exists\n",
    "    filename = root.find('filename').text\n",
    "    image_path = ORIGINAL_IMAGES_PATH / filename\n",
    "    if not os.path.isfile(image_path):\n",
    "        continue\n",
    "    \n",
    "    # get width and height of the image\n",
    "    w = int(root.find('size/width').text)\n",
    "    h = int(root.find('size/height').text)\n",
    "\n",
    "    yolo_labels = []\n",
    "    for obj in root.iter('object'):\n",
    "        # get class and append to `classes` if not exists\n",
    "        cls = obj.find('name').text\n",
    "        if cls not in classes:\n",
    "            classes.append(cls)\n",
    "\n",
    "        # get bounding box and converts to yolo format\n",
    "        idx = classes.index(cls)\n",
    "        bbox = [int(x.text) for x in obj.find('bndbox')]\n",
    "        yolo_bbox = xml_to_yolo_bbox(bbox, w, h)\n",
    "\n",
    "        bbox_string = ' '.join([str(x) for x in yolo_bbox])\n",
    "        yolo_label = f'{idx} {bbox_string}'\n",
    "\n",
    "        yolo_labels.append(yolo_label)\n",
    "\n",
    "    if yolo_labels:\n",
    "        data_id = image_path.stem\n",
    "        \n",
    "        # add a valid data to the valid list\n",
    "        images[data_id] = image_path\n",
    "        labels[data_id] = yolo_labels\n",
    "\n",
    "print(f'total valid data count: {len(labels)}/{len(label_files)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split train, test, and validation from the dataset. By split training/testing/validation data at 70/15/15%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, temp_data = train_test_split(list(labels.keys()), test_size=0.3, shuffle=True)\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.5, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the train, test, and validation data, according to the cell above, to its responding folder includes images and annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# delete all files in the current yolo folder\n",
    "for dir in [TRAIN_IMAGES_PATH, TRAIN_LABELS_PATH,\n",
    "            TEST_IMAGES_PATH, TEST_LABELS_PATH,\n",
    "            VALIDATION_IMAGES_PATH, VALIDATION_LABELS_PATH]:\n",
    "    for f in os.listdir(dir):\n",
    "        if os.path.isfile(dir / f):\n",
    "            os.remove(dir / f)\n",
    "\n",
    "# copy images and annotations to each corresponding train/test/validation folder.\n",
    "for data, dir in [(train_data, TRAIN_DIR),\n",
    "                  (val_data, VALIDATION_DIR),\n",
    "                  (test_data, TEST_DIR)]:\n",
    "    for data_id in data:\n",
    "        image_name = images[data_id].name  # get data id (file name)\n",
    "\n",
    "        # copy image to the folder\n",
    "        shutil.copy(ORIGINAL_IMAGES_PATH / image_name,\n",
    "                    YOLO_DATASET_PATH / 'images' / dir / image_name)\n",
    "        \n",
    "        # write yolo labels to a file\n",
    "        with open(YOLO_DATASET_PATH / 'labels' / dir / f'{data_id}.txt', 'w') as f:\n",
    "            f.write('\\n'.join(labels[data_id]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a YAML file for YOLO to works with, define dataset path, train/test/validation path, class count, and class names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "DATASET_YAML = YOLO_DATASET_PATH / 'face-mask-detection.yaml'\n",
    "\n",
    "content = {\n",
    "    'path': str(YOLO_DATASET_PATH),\n",
    "    'train': 'images/train',\n",
    "    'test': 'images/test',\n",
    "    'val': 'images/val',\n",
    "\n",
    "    'nc': len(classes),\n",
    "    'names': classes\n",
    "}\n",
    "with open(DATASET_YAML, 'w') as f:\n",
    "    yaml.dump(content, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the YOLOv8 weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "YOLO_WEIGHT_PATH = DATASET_PATH / 'yolov8n.pt'\n",
    "\n",
    "# https://github.com/ultralytics/assets/releases\n",
    "WEIGHT_URL = 'https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8n.pt'\n",
    "\n",
    "urlretrieve(WEIGHT_URL, YOLO_WEIGHT_PATH)\n",
    "\n",
    "print('weight downloaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the weight to the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "model = YOLO(YOLO_WEIGHT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training with dataset at 30 epochs using CUDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results = model.train(\n",
    "    data=DATASET_YAML,  # dataset\n",
    "    epochs=30,          # epochs\n",
    "    imgsz=640,          # image size\n",
    "    batch=8,            # batch size\n",
    "    device=0,           # device to train (cpu or gpu)\n",
    "    save=True           # save the model as a weight file\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick a random image from validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import cv2\n",
    "\n",
    "pick_id = random.choice(val_data)\n",
    "image = cv2.imread(VALIDATION_IMAGES_PATH / f'{pick_id}.png')  # read with opencv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference the model with the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.predict(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw classes and bounding boxes on the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "WIDTH = 2\n",
    "FONT_SCALE = 0.5\n",
    "\n",
    "for box in results[0].boxes:\n",
    "    for xyxy in box.xyxy:\n",
    "        x1, y1, x2, y2 = xyxy.cpu().numpy().astype(np.int32)  # get bounding box\n",
    "        cls = box.cls.cpu().numpy().astype(np.int32)[0]       # get class id\n",
    "        cls = classes[cls]                                    # get class name from the id\n",
    "\n",
    "        # random a color for each bounding box in BGR.\n",
    "        color = (\n",
    "            random.randint(0, 255),\n",
    "            random.randint(0, 255),\n",
    "            random.randint(0, 255)\n",
    "        )\n",
    "\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), color, WIDTH)  # draw a bounding box\n",
    "        # write a class name to the box\n",
    "        cv2.putText(image, cls, (x1, y1 - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, FONT_SCALE,\n",
    "                    color, WIDTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, displays the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# convert opencv's bgr to rgb for displaying with matplotlib\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.axis('off')  # disable plot's axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saves the image. (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite('output.png', image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
